package sentencepiece

import (
	"reflect"
	"testing"
)

func TestTokenization(t *testing.T) {
	sp, err := NewSentencepieceFromFile("test_data/xlnet-base-cased-spiece.model", false)
	if err != nil {
		t.Errorf("Unable to create sentencepiece")
		return
	}

	tests := []struct {
		text   string
		tokens []Token
	}{
		{text: "this", tokens: []Token{{ID: 52, Text: "â–this"}}},
		{text: "hello", tokens: []Token{{ID: 24717, Text: "â–hello"}}},
		{text: "This is a sample sentence to be tokeÌnized", tokens: []Token{
			{ID: 122, Text: "â–This"},
			{ID: 27, Text: "â–is"},
			{ID: 24, Text: "â–a"},
			{ID: 4561, Text: "â–sample"},
			{ID: 3833, Text: "â–sentence"},
			{ID: 22, Text: "â–to"},
			{ID: 39, Text: "â–be"},
			{ID: 22, Text: "â–to"},
			{ID: 1235, Text: "ke"},
			{ID: 0, Text: "Ì"},
			{ID: 180, Text: "n"},
			{ID: 1227, Text: "ized"},
		}},
		{text: "Wondering how this will get tokenized ğŸ¤” ?", tokens: []Token{
			{ID: 14748, Text: "â–Wonder"},
			{ID: 56, Text: "ing"},
			{ID: 160, Text: "â–how"},
			{ID: 52, Text: "â–this"},
			{ID: 53, Text: "â–will"},
			{ID: 133, Text: "â–get"},
			{ID: 17366, Text: "â–token"},
			{ID: 1227, Text: "ized"},
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "ğŸ¤”"},
			{ID: 17, Text: "â–"},
			{ID: 82, Text: "?"},
		}},
		{text: "Ä°s th!s ğ©¸½ Ïº Å Å“ UgljÅ¡iÄ‡ dáº¥u náº·ng", tokens: []Token{
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "Ä°"},
			{ID: 23, Text: "s"},
			{ID: 17, Text: "â–"},
			{ID: 138, Text: "th"},
			{ID: 136, Text: "!"},
			{ID: 23, Text: "s"},
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "ğ©¸½"},
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "Ïº"},
			{ID: 17, Text: "â–"},
			{ID: 0, Text: "Å "},
			{ID: 128, Text: "â–U"},
			{ID: 15222, Text: "gl"},
			{ID: 1315, Text: "j"},
			{ID: 0, Text: "Å¡"},
			{ID: 150, Text: "i"},
			{ID: 0, Text: "Ä‡"},
			{ID: 17, Text: "â–"},
			{ID: 66, Text: "d"},
			{ID: 0, Text: "áº¥"},
			{ID: 660, Text: "u"},
			{ID: 17, Text: "â–"},
			{ID: 180, Text: "n"},
			{ID: 0, Text: "áº·"},
			{ID: 3511, Text: "ng"},
		}},
		{text: "compose email to john saying i will be running late to office today because i am not feeling well, my head is aching and in the body add shall we meet next week and when we go to the office lets reach by around 10 am and go for a movie in the evening, may be Spiderman which seems to be a very good movie which got 5 star review from rottentomatoes and imdb", tokens: []Token{
			{ID: 23391, Text: "â–compose"},
			{ID: 1706, Text: "â–email"},
			{ID: 22, Text: "â–to"},
			{ID: 17, Text: "â–"},
			{ID: 22116, Text: "john"},
			{ID: 591, Text: "â–saying"},
			{ID: 17, Text: "â–"},
			{ID: 150, Text: "i"},
			{ID: 53, Text: "â–will"},
			{ID: 39, Text: "â–be"},
			{ID: 926, Text: "â–running"},
			{ID: 471, Text: "â–late"},
			{ID: 22, Text: "â–to"},
			{ID: 495, Text: "â–office"},
			{ID: 494, Text: "â–today"},
			{ID: 149, Text: "â–because"},
			{ID: 17, Text: "â–"},
			{ID: 150, Text: "i"},
			{ID: 569, Text: "â–am"},
			{ID: 50, Text: "â–not"},
			{ID: 1803, Text: "â–feeling"},
			{ID: 143, Text: "â–well"},
			{ID: 19, Text: ","},
			{ID: 94, Text: "â–my"},
			{ID: 291, Text: "â–head"},
			{ID: 27, Text: "â–is"},
			{ID: 24, Text: "â–a"},
			{ID: 5410, Text: "ching"},
			{ID: 21, Text: "â–and"},
			{ID: 25, Text: "â–in"},
			{ID: 18, Text: "â–the"},
			{ID: 458, Text: "â–body"},
			{ID: 1319, Text: "â–add"},
			{ID: 1530, Text: "â–shall"},
			{ID: 80, Text: "â–we"},
			{ID: 767, Text: "â–meet"},
			{ID: 244, Text: "â–next"},
			{ID: 260, Text: "â–week"},
			{ID: 21, Text: "â–and"},
			{ID: 90, Text: "â–when"},
			{ID: 80, Text: "â–we"},
			{ID: 216, Text: "â–go"},
			{ID: 22, Text: "â–to"},
			{ID: 18, Text: "â–the"},
			{ID: 495, Text: "â–office"},
			{ID: 10234, Text: "â–lets"},
			{ID: 1287, Text: "â–reach"},
			{ID: 37, Text: "â–by"},
			{ID: 199, Text: "â–around"},
			{ID: 241, Text: "â–10"},
			{ID: 569, Text: "â–am"},
			{ID: 21, Text: "â–and"},
			{ID: 216, Text: "â–go"},
			{ID: 28, Text: "â–for"},
			{ID: 24, Text: "â–a"},
			{ID: 1432, Text: "â–movie"},
			{ID: 25, Text: "â–in"},
			{ID: 18, Text: "â–the"},
			{ID: 2060, Text: "â–evening"},
			{ID: 19, Text: ","},
			{ID: 132, Text: "â–may"},
			{ID: 39, Text: "â–be"},
			{ID: 17489, Text: "â–Spider"},
			{ID: 249, Text: "man"},
			{ID: 59, Text: "â–which"},
			{ID: 1303, Text: "â–seems"},
			{ID: 22, Text: "â–to"},
			{ID: 39, Text: "â–be"},
			{ID: 24, Text: "â–a"},
			{ID: 172, Text: "â–very"},
			{ID: 195, Text: "â–good"},
			{ID: 1432, Text: "â–movie"},
			{ID: 59, Text: "â–which"},
			{ID: 345, Text: "â–got"},
			{ID: 306, Text: "â–5"},
			{ID: 1795, Text: "â–star"},
			{ID: 1398, Text: "â–review"},
			{ID: 40, Text: "â–from"},
			{ID: 28626, Text: "â–rotten"},
			{ID: 261, Text: "to"},
			{ID: 18693, Text: "mato"},
			{ID: 202, Text: "es"},
			{ID: 21, Text: "â–and"},
			{ID: 7693, Text: "â–im"},
			{ID: 66, Text: "d"},
			{ID: 508, Text: "b"},
		}},
	}

	for _, test := range tests {
		output := sp.Tokenize(test.text)
		if !reflect.DeepEqual(output, test.tokens) {
			t.Errorf("Tokenization error : %s, len %d, got %v || expected %v", test.text, len(test.text), output, test.tokens)
		}
	}
}

func TestTokenizationSPM(t *testing.T) {
	sp, err := NewSentencepieceFromFile("test_data/spm.model", true)
	if err != nil {
		t.Errorf("Unable to create sentencepiece")
		return
	}

	tests := []struct {
		text   string
		tokens []Token
	}{
		{text: "this", tokens: []Token{{ID: 48, Text: "â–this"}}},
		{text: "hello", tokens: []Token{{ID: 10975, Text: "â–hello"}}},
		{text: "This is a sample sentence to be tokeÌnized", tokens: []Token{
			{ID: 48, Text: "â–this"},
			{ID: 25, Text: "â–is"},
			{ID: 21, Text: "â–a"},
			{ID: 5717, Text: "â–sample"},
			{ID: 5123, Text: "â–sentence"},
			{ID: 20, Text: "â–to"},
			{ID: 44, Text: "â–be"},
			{ID: 20, Text: "â–to"},
			{ID: 1048, Text: "ke"},
			{ID: 1, Text: "Ì"},
			{ID: 103, Text: "n"},
			{ID: 1333, Text: "ized"},
		}},
		{text: ".", tokens: []Token{{ID: 13, Text: "â–"}, {ID: 9, Text: "."}}},
		{text: "this is a dot .", tokens: []Token{
			{ID: 48, Text: "â–this"},
			{ID: 25, Text: "â–is"},
			{ID: 21, Text: "â–a"},
			{ID: 14123, Text: "â–dot"},
			{ID: 13, Text: "â–"},
			{ID: 9, Text: "."},
		}},
		{text: "compose email to john saying i will be running late to office today because i am not feeling well, my head is aching and in the body add shall we meet next week and when we go to the office lets reach by around 10 am and go for a movie in the evening, may be Spiderman which seems to be a very good movie which got 5 star review from rottentomatoes and imdb", tokens: []Token{
			{ID: 18217, Text: "â–compose"},
			{ID: 8517, Text: "â–email"},
			{ID: 20, Text: "â–to"},
			{ID: 239, Text: "â–john"},
			{ID: 1148, Text: "â–saying"},
			{ID: 31, Text: "â–i"},
			{ID: 129, Text: "â–will"},
			{ID: 44, Text: "â–be"},
			{ID: 946, Text: "â–running"},
			{ID: 456, Text: "â–late"},
			{ID: 20, Text: "â–to"},
			{ID: 488, Text: "â–office"},
			{ID: 786, Text: "â–today"},
			{ID: 185, Text: "â–because"},
			{ID: 31, Text: "â–i"},
			{ID: 589, Text: "â–am"},
			{ID: 52, Text: "â–not"},
			{ID: 1249, Text: "â–feeling"},
			{ID: 134, Text: "â–well"},
			{ID: 15, Text: ","},
			{ID: 51, Text: "â–my"},
			{ID: 157, Text: "â–head"},
			{ID: 25, Text: "â–is"},
			{ID: 17010, Text: "â–aching"},
			{ID: 17, Text: "â–and"},
			{ID: 19, Text: "â–in"},
			{ID: 14, Text: "â–the"},
			{ID: 358, Text: "â–body"},
			{ID: 3547, Text: "â–add"},
			{ID: 3004, Text: "â–shall"},
			{ID: 95, Text: "â–we"},
			{ID: 1255, Text: "â–meet"},
			{ID: 328, Text: "â–next"},
			{ID: 877, Text: "â–week"},
			{ID: 17, Text: "â–and"},
			{ID: 76, Text: "â–when"},
			{ID: 95, Text: "â–we"},
			{ID: 162, Text: "â–go"},
			{ID: 20, Text: "â–to"},
			{ID: 14, Text: "â–the"},
			{ID: 488, Text: "â–office"},
			{ID: 6884, Text: "â–lets"},
			{ID: 1470, Text: "â–reach"},
			{ID: 34, Text: "â–by"},
			{ID: 140, Text: "â–around"},
			{ID: 332, Text: "â–10"},
			{ID: 589, Text: "â–am"},
			{ID: 17, Text: "â–and"},
			{ID: 162, Text: "â–go"},
			{ID: 26, Text: "â–for"},
			{ID: 21, Text: "â–a"},
			{ID: 1308, Text: "â–movie"},
			{ID: 19, Text: "â–in"},
			{ID: 14, Text: "â–the"},
			{ID: 2089, Text: "â–evening"},
			{ID: 15, Text: ","},
			{ID: 123, Text: "â–may"},
			{ID: 44, Text: "â–be"},
			{ID: 5650, Text: "â–spider"},
			{ID: 177, Text: "man"},
			{ID: 56, Text: "â–which"},
			{ID: 2206, Text: "â–seems"},
			{ID: 20, Text: "â–to"},
			{ID: 44, Text: "â–be"},
			{ID: 21, Text: "â–a"},
			{ID: 253, Text: "â–very"},
			{ID: 254, Text: "â–good"},
			{ID: 1308, Text: "â–movie"},
			{ID: 56, Text: "â–which"},
			{ID: 330, Text: "â–got"},
			{ID: 331, Text: "â–5"},
			{ID: 778, Text: "â–star"},
			{ID: 1487, Text: "â–review"},
			{ID: 37, Text: "â–from"},
			{ID: 11573, Text: "â–rotten"},
			{ID: 262, Text: "to"},
			{ID: 8844, Text: "mato"},
			{ID: 160, Text: "es"},
			{ID: 17, Text: "â–and"},
			{ID: 797, Text: "â–im"},
			{ID: 9007, Text: "db"},
		}},
	}

	for _, test := range tests {
		output := sp.Tokenize(test.text)
		if !reflect.DeepEqual(output, test.tokens) {
			t.Errorf("Tokenization error : %s, len %d, got %v || expected %v", test.text, len(test.text), output, test.tokens)
		}
	}
}

func TestControlWords(t *testing.T) {
	sp, err := NewSentencepieceFromFile("test_data/xlnet-base-cased-spiece.model", false)
	if err != nil {
		t.Errorf("Unable to create sentencepiece")
		return
	}

	unknownIndex := sp.GetUnknownIndex()
	if unknownIndex != 0 {
		t.Errorf("Unknown index not equal to 0")
	}

	clsIndex, ok := sp.GetControlWord("<cls>")
	if !ok || clsIndex != 3 {
		t.Errorf("Control word [CLS] not correct : %d", clsIndex)
	}

}

func TestControlWords2(t *testing.T) {
	sp, err := NewSentencepieceFromFile("test_data/spm.model", true)
	if err != nil {
		t.Errorf("Unable to create sentencepiece")
		return
	}

	unknownIndex := sp.GetUnknownIndex()
	if unknownIndex != 1 {
		t.Errorf("Unknown index not equal to 1")
	}

	clsIndex, ok := sp.GetControlWord("[CLS]")
	if !ok || clsIndex != 2 {
		t.Errorf("Control word [CLS] not correct")
	}
}

func TestRunLengthchange(t *testing.T) {
	testRunLengthchange(t, "Why would you make changes here? Did you want just model generated?")
	testRunLengthchange(t, "Ä°s th!s ğ©¸½ Ïº Å Å“ UgljÅ¡iÄ‡ dáº¥u náº·ng")
	testRunLengthchange(t, "æ˜¨æ—¥ã€å‹é”ã¨æ˜ ç”»ã‚’è¦‹ã¾ã—ãŸã€‚")
	testRunLengthchange(t, "compose email to john saying i will be running late to office today because i am not feeling well, my head is aching and in the body add shall we meet next week and when we go to the office lets reach by around 10 am and go for a movie in the evening, may be Spiderman which seems to be a very good movie which got 5 star review from rottentomatoes and imdb")
	testRunLengthchange(t, "æˆ‘æƒ³å­¦ä¹ æ±‰è¯­ï¼Œå› ä¸ºæˆ‘è§‰å¾—å®ƒå¾ˆæœ‰ç”¨!")
}

func testRunLengthchange(t *testing.T, text string) {
	originalLen := len([]rune(text))
	text = normalize(text)

	lenAfterNorm := len([]rune(text))
	if originalLen != lenAfterNorm {
		t.Errorf("text length %d changed after normalization: %d", originalLen, lenAfterNorm)
	}
	runes := torunes(text)
	padding := len(runes) - originalLen
	lenAfterPadding := len(runes)
	if padding != 0 && padding != 1 {
		t.Errorf("padding shoudl be 0 or 1")
	}
	replaceWhiteSpace(runes)
	if len(runes) != lenAfterPadding {
		t.Errorf("replacing white space shouldn't change length")
	}
}

func TestTokenOffset(t *testing.T) {
	spm, err := NewSentencepieceFromFile("test_data/spm1.model", false)
	if err != nil {
		t.Errorf("Unable to create sentencepiece: %v", err)
	}
	testTokenOffset(t, spm, "replacing white space shouldn't change length")
	testTokenOffset(t, spm, "correct. I think if a token isn't in the dictionary, if uses the special token UNK which has some set value")
	testTokenOffset(t, spm, "correct. æ¥é€±ã®é‡‘æ›œæ—¥ã«ä¼šã„ã¾ã—ã‚‡ã†ã€‚")
	testTokenOffset(t, spm, "Whaaaaaaat is thaaa!t you can see????")
	testTokenOffset(t, spm, "compose email to john saying i will be running late to office today because i am not feeling well, my head is aching and in the body add shall we meet next week and when we go to the office lets reach by around 10 am and go for a movie in the evening, may be Spiderman which seems to be a very good movie which got 5 star review from rottentomatoes and imdb")
	testTokenOffset(t, spm, "æˆ‘æƒ³å­¦ä¹ æ±‰è¯­ï¼Œå› ä¸ºæˆ‘è§‰å¾—å®ƒå¾ˆæœ‰ç”¨!")
}

func testTokenOffset(t *testing.T, spm Sentencepiece, text string) {
	tokenOffsets := spm.TokenizeToOffsets(text)
	runes := spm.prepareFortokenize(text)
	padding := false
	if len(runes)-len([]rune(text)) == 1 {
		runes = runes[1:]
		padding = true
	}
	for i, offset := range tokenOffsets {
		word := string(runes[offset.Start:offset.End])
		if i == 0 && padding {
			tempRunes := []rune(offset.Text)
			offset.Text = string(tempRunes[1:])
		}
		if offset.Text != word {
			t.Errorf("%d %s != %s", i, offset.Text, word)
		}
	}

}

func BenchmarkSentencePiece(b *testing.B) {
	sp, err := NewSentencepieceFromFile("test_data/xlnet-base-cased-spiece.model", false)
	if err != nil {
		b.Errorf("Unable to create sentencepiece")
		return
	}

	b.ResetTimer()

	inputs := []string{
		"compose email to john saying i will be running late to office today because i am not feeling well, my head is aching and in the body add shall we meet next week and when we go to the office lets reach by around 10 am and go for a movie in the evening, may be Spiderman which seems to be a very good movie which got 5 star review from rottentomatoes and imdb",
	}

	for _, input := range inputs {
		b.Run(firstNChars(input, 20), func(b *testing.B) {
			for i := 0; i < b.N; i++ {
				sp.Tokenize(input)
			}
		})
	}
}

func firstNChars(s string, n int) string {
	if len(s) < n {
		return s
	}
	return s[:n]
}
